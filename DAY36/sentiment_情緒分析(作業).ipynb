{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_情緒分析(作業).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8txPkmLtdT0A"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from future.utils import iteritems\n",
        "from builtins import range"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xVfwnFOdmAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbbdad6e-8eac-4cf2-e5ea-769fb25edb89"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet') \n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pph9cFIgdogG"
      },
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
        "stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n",
        "\n",
        "# 另一個 stopwords 的來源\n",
        "# from nltk.corpus import stopwords\n",
        "# stopwords.words('english')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE7vjMfyd37o"
      },
      "source": [
        "# 讀正向與負向 reviews\n",
        "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
        "positive_reviews = BeautifulSoup(open('./positive.review', encoding='utf-8').read(), features=\"html5lib\")\n",
        "positive_reviews = positive_reviews.findAll('review_text')\n",
        "\n",
        "negative_reviews = BeautifulSoup(open('./negative.review', encoding='utf-8').read(), features=\"html5lib\")\n",
        "negative_reviews = negative_reviews.findAll('review_text')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfiYwxRaiATe"
      },
      "source": [
        "# 基於nltk自建 tokenizer\n",
        "\n",
        "def my_tokenizer(s):\n",
        "    s = s.lower() # downcase\n",
        "    tokens = nltk.tokenize.word_tokenize(s) # 將字串改為tokens\n",
        "    tokens = [t for t in tokens if len(t) > 2] # 去除短字\n",
        "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # 去除大小寫\n",
        "    tokens = [t for t in tokens if t not in stopwords] # 去除 stopwords\n",
        "    return tokens"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d1mSBCmilJm"
      },
      "source": [
        "# 先產生 word-to-index map 再產生 word-frequency vectors\n",
        "# 同時儲存 tokenized 版本未來不需再做 tokenization\n",
        "word_index_map = {}\n",
        "current_index = 0\n",
        "positive_tokenized = []\n",
        "negative_tokenized = []\n",
        "orig_reviews = []"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8mbigv9ixQp"
      },
      "source": [
        "for review in positive_reviews:\n",
        "    orig_reviews.append(review.text)\n",
        "    tokens = my_tokenizer(review.text)\n",
        "    positive_tokenized.append(tokens)\n",
        "    for token in tokens:\n",
        "        if token not in word_index_map:\n",
        "            word_index_map[token] = current_index\n",
        "            current_index += 1"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEn2CIF0i_0d"
      },
      "source": [
        "for review in negative_reviews:\n",
        "    orig_reviews.append(review.text)\n",
        "    tokens = my_tokenizer(review.text)\n",
        "    negative_tokenized.append(tokens)\n",
        "    for token in tokens:\n",
        "        if token not in word_index_map:\n",
        "            word_index_map[token] = current_index\n",
        "            current_index += 1\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGZtDtaHjQVO",
        "outputId": "a1a7b532-d74e-45e2-cfd8-8b00faf63ed5"
      },
      "source": [
        "print(\"len(word_index_map):\", len(word_index_map))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(word_index_map): 21531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu7lYgq8jS1r"
      },
      "source": [
        "# now let's create our input matrices\n",
        "def tokens_to_vector(tokens, label):\n",
        "    x = np.zeros(len(word_index_map) + 1) # 最後一個元素是標記\n",
        "    for t in tokens:\n",
        "        i = word_index_map[t]\n",
        "        x[i] += 1\n",
        "    x = x / x.sum() # 正規化數據提升未來準確度\n",
        "    x[-1] = label\n",
        "    return x"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbV7UF2AkUqE"
      },
      "source": [
        "N = len(positive_tokenized) + len(negative_tokenized)\n",
        "# (N x D+1) 矩陣 - 擺在一塊將來便於shuffle\n",
        "data = np.zeros((N, len(word_index_map) + 1))\n",
        "i = 0\n",
        "for tokens in positive_tokenized:\n",
        "    xy = tokens_to_vector(tokens, 1)\n",
        "    data[i,:] = xy\n",
        "    i += 1\n",
        "\n",
        "for tokens in negative_tokenized:\n",
        "    xy = tokens_to_vector(tokens, 0)\n",
        "    data[i,:] = xy\n",
        "    i += 1\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQlcBvgLjbMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "848dab0c-3e54-46bf-d3f5-7e2dc5fde2fd"
      },
      "source": [
        "# shuffle data 創造 train/test splits\n",
        "# 多次嘗試!\n",
        "orig_reviews, data = shuffle(orig_reviews, data)\n",
        "\n",
        "X = data[:,:-1]\n",
        "Y = data[:,-1]\n",
        "\n",
        "# 最後 100 列是測試用\n",
        "Xtrain = X[:-100,]\n",
        "Ytrain = Y[:-100,]\n",
        "Xtest = X[-100:,]\n",
        "Ytest = Y[-100:,]\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(Xtrain, Ytrain)\n",
        "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
        "print(\"Test accuracy:\", model.score(Xtest, Ytest))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy: 0.7078947368421052\n",
            "Test accuracy: 0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYMdDcOMjhcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ccdea3-9bb2-4217-8b60-4bf9cb71ea05"
      },
      "source": [
        "# 列出每個字的正負 weight\n",
        "# 用不同的 threshold values!\n",
        "threshold = 0.5\n",
        "for word, index in iteritems(word_index_map):\n",
        "    weight = model.coef_[0][index]\n",
        "    if weight > threshold or weight < -threshold:\n",
        "        print(word, weight)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this 0.8886807773354368\n",
            "wa -2.662283827006473\n",
            "read 0.9690227177354345\n",
            "ha 0.6365814053538728\n",
            "doe -0.6676935311234203\n",
            "recommend 1.0239485339975183\n",
            "book -0.803998387420233\n",
            "excellent 0.9557917355248554\n",
            "boring -0.8930656563570697\n",
            "page -0.6815042725658661\n",
            "you 1.4403677528727212\n",
            "n't -2.5080885330884297\n",
            "then -0.7884822322220709\n",
            "money -0.5247592456974569\n",
            "easy 0.9447424462293644\n",
            "writing -0.5930508255095489\n",
            "wonderful 0.6069947877869759\n",
            "love 1.0499038611640001\n",
            "life 0.7633609790159772\n",
            "... -1.1329902464752108\n",
            "highly 0.9668054474138137\n",
            "enjoyed 0.5377193908551716\n",
            "loved 0.789004770815162\n",
            "instead -0.6335507917560298\n",
            "favorite 0.6227857972378267\n",
            "bad -1.1089567939602059\n",
            "waste -0.881022752675318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cQ5ZMydklO2",
        "outputId": "fdc9ca4a-e7b6-43ab-f0a1-d6c41a6a44df"
      },
      "source": [
        "# 找出歸類錯誤的例子\n",
        "preds = model.predict(X)\n",
        "P = model.predict_proba(X)[:,1] # p(y = 1 | x)\n",
        "\n",
        "# 只列出最糟的\n",
        "minP_whenYis1 = 1\n",
        "maxP_whenYis0 = 0\n",
        "wrong_positive_review = None\n",
        "wrong_negative_review = None\n",
        "wrong_positive_prediction = None\n",
        "wrong_negative_prediction = None\n",
        "for i in range(N):\n",
        "    p = P[i]\n",
        "    y = Y[i]\n",
        "    if y == 1 and p < 0.5:\n",
        "        if p < minP_whenYis1:\n",
        "            wrong_positive_review = orig_reviews[i]\n",
        "            wrong_positive_prediction = preds[i]\n",
        "            minP_whenYis1 = p\n",
        "    elif y == 0 and p > 0.5:\n",
        "        if p > maxP_whenYis0:\n",
        "            wrong_negative_review = orig_reviews[i]\n",
        "            wrong_negative_prediction = preds[i]\n",
        "            maxP_whenYis0 = p\n",
        "\n",
        "print(\"Most wrong positive review (prob = %s, pred = %s):\" % (minP_whenYis1, wrong_positive_prediction))\n",
        "print(wrong_positive_review)\n",
        "print(\"Most wrong negative review (prob = %s, pred = %s):\" % (maxP_whenYis0, wrong_negative_prediction))\n",
        "print(wrong_negative_review)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most wrong positive review (prob = 0.3335232926051159, pred = 0.0):\n",
            "\n",
            "This book was hard to put down.Can't wait for the next book\n",
            "\n",
            "Most wrong negative review (prob = 0.5947081304393999, pred = 1.0):\n",
            "\n",
            "This seemed too long and too drawn out\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZkSH_RGkrFi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}